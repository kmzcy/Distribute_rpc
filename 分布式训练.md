

## 项目设计

### 亮点设计

1. 提升代码质量，体现：做好测试，
2. 单元测试的覆盖率，测试用例，开源的测试框架。
3. 上线，部署到服务器上。
4. 有界面，展示项目的运行情况。
5. 给自己的项目压测，开源的压测框架。发现我们的性能瓶颈。修复性能瓶颈。qps/tps提升了多少倍。（制造性能瓶颈，缓存的使用与否？）

### 难点设计

1. 制造难点
2. FGC/CPU飙升的一个问题排查。FGC: 埋藏一个内存泄漏的bug，看博客找一个bug植入系统内，mock一些流量。然后按照排查套路进行排查：DUMP内存，看一下堆中的各个对象的大小，找到内存中是哪个对象过大导致一直占用内存，定位之后如何解决。压测：https://segmentfault.com/a/1190000020211494 
3. jvm(java虚拟机)频繁FGC导致cpu占用过高：https://blog.csdn.net/liaomingwu/article/details/118962901  https://www.cnblogs.com/zjdxr-up/p/15800225.html
4. 并发问题：并发的访问一个不支持并发的集合，伪造？--> 死锁：围绕一个死锁现场进行排查，或，并发集合的问题，因为设计的疏忽没有用并发集合 --> 聊一下并发集合：准备好并发集合的东西，原子类的源码，死锁：写一个死锁？提前准备好死锁的代码。
5. 流量突增：蓄洪，泄洪，制造问题解决问题。

JVM参数设置：https://zhuanlan.zhihu.com/p/117627812

OOM：故障排查：https://blog.csdn.net/qiubboy/article/details/117965200

## 分布式训练

单机多卡拓展到分布式（本质上并没有区别）

单机多卡和分布式训练的区别

1. 数据放在分布式的文件系统中，而不是放在机器本地，所有的机器都能够读取样本。
2. 一般有多个worker，多台机器
3. 多个参数服务器

![image-20230716214855291](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230716214855291.png)

GPU的机器架构：

![image-20230716230745035](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230716230745035.png)

单机的情况下：GPU和GPU之间的带宽是较好的，GPU到CPU的速度会相对降低，而跨机器要通过交换机速度会更进一步降低。因此机器之间的通讯会较慢。所以性能要好，尽量本地之间多通讯

一般来说在GPU里面再做成一个sever

![image-20230717002050433](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230717002050433.png)

在GPU里面再加一层sever尽量通过这层sever对数据进行处理，能够较大的提升整体性能。

样例：样本读进来，分机器和卡拿到自己所需的样本。机器接下来从参数服务器拿到新的模型， 但是我们的模型不能直接复制到每个GPU上面，因为这样速度较慢的通信会发生四次。我们应该先将整个参数拿到主内存中（即运行在GPU间的sever上），再将模型进行分发。每个GPU算自己的梯度后，在本地做allreduce，即所有的GPU数据汇总，相加，在这之后再发回服务器。服务器对收取到的某个梯度求和，更新整个梯度。

![image-20230717003829291](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230717003829291.png)

1. 每个worker（GPU）都同步计算一个批量，成为同步SGD
2. n个GPU，每个GPU每次处理b个样本，同步SGD相当于在单GPU运行批量大小为nb的SGD
3. 理想情况下获得n倍的加速。





![image-20230718143644828](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230718143644828.png)

![image-20230718143800826](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230718143800826.png)

![image-20230718145114451](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230718145114451.png)

对于多机多卡。需要在每个节点上运行如上指令，参数解释：-nnodes：共有多少台机子（节点） node_rank：代表运行在哪个节点上 

![image-20230718151232752](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230718151232752.png)

分布式训练 https://blog.csdn.net/weixin_44966641/article/details/121872773

## 参考资料（将模型部署到spring框架上）

1. https://blog.csdn.net/2201_75499313/article/details/128160480
2. https://blog.csdn.net/qq_33283652/article/details/118574227
3. https://zhuanlan.zhihu.com/p/620192598
4. https://blog.csdn.net/jerry11112/article/details/108166689
5. https://blog.csdn.net/m0_46503651/article/details/106974051?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-106974051-blog-108166689.235%5Ev38%5Epc_relevant_default_base&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-106974051-blog-108166689.235%5Ev38%5Epc_relevant_default_base&utm_relevant_index=1
6. https://blog.csdn.net/m0_46503651/article/details/108555082
7. https://blog.csdn.net/m0_61789994/article/details/130699370?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-130699370-blog-108166689.235%5Ev38%5Epc_relevant_default_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-130699370-blog-108166689.235%5Ev38%5Epc_relevant_default_base&utm_relevant_index=5

## 思路

flask、tornado（用python的web框架 flask。tornado等搭一个微服务，和java服务之间使用接口调用）

ONNXRuntime（https://zhuanlan.zhihu.com/p/582974246）

分发模型，分布计算梯度， 发回终端进行汇总



远程调用 不同的服务放在不同的机器上spring cloud http+json : 天然的跨平台（满足希望卡平台使用显卡的需求）

负载均衡：订单服务需要商品服务，每一个节点都有商品服务，订单服务可以去任意机器查询，不能使得任意一台服务器太忙，也不能使得任意一台服务器太闲。算法：轮询，最小连接：优先选择连接数最小的，散列：同一用户的请求被分配到同意服务器上。（每个计算节点都有空闲和忙碌之分，同样需要类似的东西去进行调度）

服务注册：a调用b，但是a并不知道b在哪台服务器上有，哪些正常，哪些已经下线，为了解决这个问题，引入注册中心。（注册中心可以用于检测显卡是否可用，当获得一个任务后，会将训练任务分散到可用的显卡上）

配置中心：每一个服务都会有大量的配置，并且每个服务都可能部署在多个机器上，我们经常需要变更配置，让每个服务在配置中心获取自己的配置。配置中心集中的管理微服务的配置。

服务的熔断和降级：服务之间相互依赖防止一个服务不可用时造成的雪崩效应。熔断：当被调用的服务经常失败，达到某个阈值时，启动熔断机制，不再去调用这个服务。服务降级：运维期间使非核心业务停机或简单处理，（熔断可用、降级）

API网关：抽象了服务中需要的公共功能，提供了负载均衡、服务自动熔断、灰度发布、统一认证、限流限控、日志统计等丰富的功能。例子：防止后台一下收到过多的请求。



![image-20230723113235447](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230723113235447.png)

![image-20230723113309647](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230723113309647.png)

## 简易RPC框架：

![image-20230727232444477](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230727232444477.png)

RPC简单的讲就是：客户端调用服务端执行，如上图所示：在服务端，本地调用了一个func函数并传入了一个x的值作为参数，调用这个函数我们期待获得一个返回值。然而我们调用的这个函数并不是在本地执行的。而是通过网络传输的方法告诉服务器我们需要调用func方法，参数是9，在服务端收到这个请求后，服务端会调用在服务端的func函数的实现，如上图所示：这个实现是return x+1。服务器在调用完成后把返回值返回服务器.

在go语言实现的gRPC中，要使用rpc我们首先应该定义一个hello.proto文件，这个文件是接口描述语言（IDL）,这个文件规定了服务器方法的方法名、参数、返回值。客户端和服务端通过接口描述语言就知道他们之间要怎么调用相关的接口了。

服务端代码：收到客户端请求，拿到需要调用的函数（SayHello）+参数（HelloRequest），调用本地的函数实现，将返回值（HelloResponse）返回给客户端

![image-20230729014218811](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230729014218811.png)

上面的代码为服务端的实现接受返回的部分的代码，它接受一个hellowrequest，然后取出其中的 name 与 前面的hellow字符串进行拼接，作为HellowResponse返回给客户端，这说明内容是包装在某个结构中的，也可以说是包中，可以被看作是一种协议，就像是网络传输中的层层包装一样。服务端具体的事情就是监听本地的端口、创建RPC服务、注册服务实现。 监听放到请求，就会自动执行实现部分的代码，并将返回值包装起来返回给客户端。

客户端代码：调用函数（SayHello），传入参数（HelloRequest），获得返回值（HelloResponse）



总结：

proto文件（IDL）定义方法、参数、返回值，使用工具直接生成go可以用的库。

Server：创建**grpcServer对象**，注册服务（也就是自己定义的方法，针对proto文件方法的实现），**启动服务器**。

Client：创建**grpcClient对象（给出服务ip+port地址）**，用grpcClient对象调用服务（也就是调用proto文件里面声明的方法），然后打印返回的结果。



java:

框架：

1. 接口文件，定义了方法名、参数、返回值，供客户端和服务端使用
2. RPC框架，让客户端和服务端可以使用框架，达到**本地调用，远端执行**的目的

测试：

1. 写一个TestServer可执行程序，注册方法的实现后，使用rpcServer进行监听并处理rpc调用
2. 写一个TestClient可执行程序，使用rpcClient进行发起rpc调用，并打印rpc调用结果

![image-20230727235252923](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230727235252923.png)

try后接括号：**从java1.7版本开始,支持使用try后面跟随（）括号管理释放资源,前提是这些可关闭的资源必须实现 java.lang.AutoCloseable 接口。**这些资源在执行完try-catch后进行.

![image-20230728233740633](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230728233740633.png)



问题：如何在客户端本地调用了相关的方法后，使得远端调用看起来就像本地调用一样，整个通知的流程是怎么走的？

文件的结构如上图：main里面是源代码，test里面是测试用例

core：框架的核心代码

### 客户端实现（动态代理）

客户端方面，客户端本地只有IDL.Hello中的内容，没有方法的具体实现，也就是说要调用一个没有实现的接口，显然，我们使用Java反射的动态代理特性，实例化一个接口，将调用接口方法“代理”给InvocationHandler中的invoke来执行，在Invoke中获取到接口名、方法名等包装成Rpc协议，发送给服务端，然后等待服务端返回。

IDL: 相当于接口描述语言，Hellowservice中定义的是服务器端提供的服务，以一个接口的方式呈现，服务端实现这些接口，客户端只管调用这些接口。HellowResponse是服务器返回的类，HellowRequest是客户端请求的类，Hellowservice中的两个方法均应该返回HellowResponse类，这三个文件规定了客户端和服务器端之间交换信息的格式，因此对于两端都应该是可见的。这个是属于rpc协议中body的内容。

core.rpc_protocol 中的两个文件则规定了rpc请求报文和返回报文的格式，这里都是一个head加上body，再明确一遍，body中装的就是我们之前规定的回应对象和请求对象，以及相关信息，这些被封装为一个新的对象，它在codec中被定义。

codec中则是存放的是body对象

![image-20230729023433545](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230729023433545.png)

请求请求的是方法，因此要有方法信息

返回返回的是参数，因此有一个参数就只返回一个object就行

IDL里面的东西其实就在object里面

![image-20230729023400661](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230729023400661.png)

![image-20230729140343827](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230729140343827.png)

我们可以看到：通过反射制造的helloService对象 hellowservice 其调用hello 方法时传入的就是hellorequest，而接收到的就是helloresponce，hellow（hi）方法本身是没有实现的，或者说HelloService 中的每个方法都没有实现。调用他们都会转移到我们的 procy代理类的方法，而代理类实际做的事情就是把要传的方法，传入的参数告诉服务器，并从服务器取回返回值。

### 服务端实现（反射调用）

服务端方面，本地需要实现接口的方法，然后在启动监听网络之前注册所有的接口，当消息到来的时候，根据RpcRequestBody中的接口名拿到接口对象，然后用反射的方式调用即可，将调用结果包装成RpcResponse，发送给客户端。

AtomicInteger: https://blog.csdn.net/fanrenxiang/article/details/80623884 / https://zhuanlan.zhihu.com/p/87141904 / 

线程池：https://blog.csdn.net/qq_40093255/article/details/116990431

## 分布式系统

我认为整体上来讲要做的东西就是一个分布式系统，那学习分布式系统有助于设置一些问题然后解决相关问题，使得整个项目更有层次性。显得更加真实。但是目前时间不够。因此我们的规划是：首先先看完java guide中rpc相关的部分补足一下项目。

整个系统可以被分为两个阶段的实现目标，第一个阶段：简单的调用深度学习服务 + 系统中的每个机器都能够拥有完整的模型，仅仅传递梯度，在请求服务的主机上加和梯度，更新模型，最后将模型分配到每个主机上进行下一步训练。

每个节点开始运行时会向注册中心注册服务，即将自己的地址信息（ip，端口以及服务等组合信息）上报给注册中心注册中心负责将地址信息保存起来。这就是注册服务。注册服务能够帮助用户自动发现服务，同时提供服务的服务器如果出现问题，也可以进行自动下线，不需要一个一个的维护配置文件。

![image-20230816164003509](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230816164003509.png)

![image-20230816164241251](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230816164241251.png)

### 注册中心的选择

为什么选择zooKeeper作为注册中心：根据CAP理论，分布式架构需要满足三种条件：一致性：所有节点在同一时间具有相同的数据。可用性：保证每个请求都能够获得响应，分割容忍：系统中的任意信息丢失或失败都不会影响系统的正常运作。

- 如果C是第一需求的话，那么会影响A的性能，因为要数据同步，不然请求结果会有差异，但是数据同步会消耗时间，期间可用性就会降低。
- 如果A是第一需求，那么只要有一个服务在，就能正常接受请求，但是对于返回结果变不能保证，原因是，在分布式部署的时候，数据一致的过程不可能想切线路那么快。
- 再如果，同时满足一致性和可用性，那么分区容错就很难保证了，也就是单点，也是分布式的基本核心。

我认为我这个系统更重要的应该是C和P，即保证一致性和分割容忍性，一致性主要在于分割训练需要同步目前模型的信息。

“作为一个分布式协同服务，ZooKeeper非常好，但是对于Service发现服务来说就不合适了，因为对于Service发现服务来说就算是返回了包含不实的信息的结果也比什么都不返回要好。” 这部分是一篇文章中作为ZooKeeper的缺点提出的，我们的系统恰恰相反，每个主机作为训练段能够提供的服务都是相同的，不需要我们容忍注册中心返回的是几分钟以前的注册信息。**Zookeeper的核心算法是ZAB，所有设计都是为了强一致性**。这个对于分布式协调系统，完全没没有毛病，我们的系统本质上讲就是一个分布式协调系统。(分布式协调技术主要用来解决分布式环境当中的多个进程之间的同步控制，让他们有序的去访问某种临界资源，防止造成脏数据的后果)

注册中心的选择：https://mp.weixin.qq.com/s?__biz=Mzg3OTU5NzQ1Mw==&mid=2247486918&idx=1&sn=5651cd0b4b9c8e68bcfa55c00c0950d6&chksm=cf034f24f874c632511684057337a744c54702543ec3690aa06dbf4bbaf980b2828f52276c9b&scene=21#wechat_redirect

### Paxos与Raft（优化项）

解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 **原子性问题** 。

ZooKeeper中已经对相关的东西进行了实现，用于管理zookeeper服务集群，跟我们的注册中心没有关系，如果我们需要这个得自己实现。

| **角色** |                           **说明**                           |
| :------: | :----------------------------------------------------------: |
|  Leader  | 为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 |
| Follower | 为客户端提供读服务，如果是写服务则转发给 Leader。参与选举过程中的投票。 |
| Observer | 为客户端提供读服务，如果是写服务则转发给 Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。 |

**选举结束后的流程**

某节点超过半数投票先当选准leader，发现阶段：每个follower 向准leader 同步 followers 最近接收的事务提议, 同步阶段：leader整理同步上一阶段收到的事务提议。最后是广播阶段：正式对外提供事务服务，并且 leader 可以进行消息广播

**领袖有任期吗？领袖的责任是什么？选举定时器怎么实现？如何确定领袖的身份，即领袖如何履行自己的职责？**

复制状态机的基本思想是一个分布式的状态机，系统由多个复制单元组成，每个复制单元均是一个状态机，它的状态保存在操作日志中。状态机的操作根据日志进行变化。服务器上的一致性模块负责接收外部命令，然后追加到自己的操作日志中，它与其他服务器上的一致性模块进行通信，以保证每一个服务器上的操作日志最终都以相同的顺序包含相同的指令。一旦指令被正确复制，那么每一个服务器的状态机都将按照操作日志的顺序来处理它们，然后将输出结果返回给客户端。这样可以保证每个服务器所执行的指令和顺序都是相同的。（可以理解为平行加速任务）

数据同步流程，借鉴了“复制状态机”的思想，都是先“提交”，再“应用”。

当Client发起数据更新请求，请求会先到领袖节点C，节点C会更新日志数据，然后通知群众节点也更新日志，当群众节点更新日志成功后，会返回成功通知给领袖C，至此完成了“提交”操作；

当领袖C收到通知后，会更新本地数据，并通知群众也更新本地数据，同时会返回成功通知给Client，至此完成了“应用”操作，如果后续Client又有新的数据更新操作，会重复上述流程。

“提交”更新日志，“应用”更新数据。

**过半机制是如何防止脑裂现象产生的？**

ZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。

**在我们的系统之中**

对于同步式锁的一些设想：为什么要加锁，每个服务器都会读取新的模型参数，在读取新的模型参数的时候模型参数不能进行修改，服务器的训练速度不统一，我们需要保证应该在一个batch中的操作统一在一个batch中。

raft协议可以用来实现我们的设想：把程序分发到多台机子上，这些机子自己构成一个集群，通过raft协议选取一个注册中心，运行注册中心的相关职责，这样不需要一个固定的注册中心，做到轻量的即插即用。



### 序列化以及序列化协议选择

序列化：对象转化二进制字节流。反序列化：二进制字节流转化为对象。

对象在网络中传播需要先进行序列化然后进行反序列化

对象存储到文件中需要先进行序列化然后读取时进行反序列化

**序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中**

java本身自带序列化方式，这种方式只需要类实现Serializabel接口即可，通过查看源代码我们可以发现java自带的Hashtable类也可以进行序列化，其中有字段：*serialVersionUID* 这个字段为私有的静态final long 型变量，在反序列化时会进行检查，检查反序列化的对象内的和类内的是否一致，静态变量本身不会序列化，这个序列化只是表象。

自带的序列化本身存在几个问题

1. 不支持跨语言调用
2. 性能较差
3. 反序列化漏洞

序列化我们选取kryo来对对象进行序列化，kryo的性能要远高于java本身自带的序列化方法。

kryo本身线程不安全，因此我们需要使用ThreadLocal来对kyro对象进行处理。

**ThreadLocal的实现是这样的：**

每个Thread维护一个 ThreadLocalMap映射表，这个映射表的key是 ThreadLocal实例本身，value是真正需要存储的Object。ThreadLocalMap本身是定义在ThreadLocal中的一个内部类，map中存储的元素为entry

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
            /** The value associated with this ThreadLocal. */
            Object value;

            Entry(ThreadLocal<?> k, Object v) {
                super(k);
                value = v;
            }
        }
```

key被设置成弱引用主要是因为ThreadlocalMap是和线程绑定在一起的，如果这样线程没有被销毁，而我们又已经不会再某个threadlocal引用，那么key-value的键值对就会一直在map中存在，这对于程序来说，就出现了内存泄漏。

为了避免这种情况，只要将key设置为弱引用，那么当发生GC的时候，就会自动将弱引用给清理掉，也就是说：假如某个用户A执行方法时产生了一份threadlocalA，然后在很长一段时间都用不到threadlocalA时，作为弱引用，它会在下次垃圾回收时被清理掉。
关于弱引用，还有一个有意思的东西：

```java
public static void main(String[] args) throws Exception {
        Map<WeakReference<Integer>, WeakReference<Integer>> map = new HashMap<>(8);
        // 注意这里~
        WeakReference<Integer> key = new WeakReference<>(1);
        WeakReference<Integer> value = new WeakReference<>(127); //改成128就会被GC回收掉
        map.put(key,value);
        System.out.println("put success");
        Thread.sleep(1000);
        System.gc();
        System.out.println("get " + map.get(key).get());
    }
```

为什么**-128 ~ 127之间**GC不会发生作用呢，肯定是因为存在一个强引用，那这个强引用在哪呢？因为Integer类中存在一个私有的静态类：IntegerCache，它其实是一个Integer类型的数组，数组存贮值为-128~127之间的Integer对象。当我们调用valueOf方法（自动装箱时）创建Integer对象时，首先查看数值i是否在-128~127之间，如果在，就返回**IntegerCache.cache中存储的对象**，如果**不在，就新创建一个对象**，然后返回。在上面的代码中值为-128-127之间时，其实在Integer类中存在它的强引用。

`ThreadLocal`的实现是这样的：每个`Thread` 维护一个 `ThreadLocalMap` 映射表，这个映射表的 `key`是 `ThreadLocal` 实例本身，`value` 是真正需要存储的 `Object`。

也就是说 `ThreadLocal` 本身并不存储值，它只是作为一个 `key` 来让线程从 `ThreadLocalMap` 获取 `value`。值得注意的是图中的虚线，表示 `ThreadLocalMap` 是使用 `ThreadLocal` 的弱引用作为 `Key`的，弱引用的对象在 GC 时会被回收。

`ThreadLocalMap`使用`ThreadLocal`的弱引用作为`key`，如果一个`ThreadLocal`没有外部强引用来引用它，那么系统 GC 的时候，这个`ThreadLocal`势必会被回收，这样一来，`ThreadLocalMap`中就会出现`key`为`null`的`Entry`，就没有办法访问这些`key`为`null`的`Entry`的`value`，如果当前线程再迟迟不结束的话，这些`key`为`null`的`Entry`的`value`就会一直存在一条强引用链：`Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value`永远无法回收，造成内存泄漏。

为了防止出现这一问题，我们应每次使用完`ThreadLocal`，都调用它的`remove()`方法，清除数据。

回到kryo，简单的使用kyro的方法如下：

```java
public class KryoSerializer implements Serializer {
    /**
     * 每个thread都拥有一个自己的kryo，保证其线程安全性。
     */
    private final ThreadLocal<Kryo> kryoThreadLocal = ThreadLocal.withInitial(() -> {
        Kryo kryo = new Kryo();
        kryo.register(RpcRequest.class);
        kryo.register(RpcResponse.class);
        return kryo;
    });

    @Override
    public byte[] serialize(Object obj) {
        try(ByteArrayOutputStream byteArrayInputStream = new ByteArrayOutputStream();
            Output output = new Output(byteArrayInputStream);)
        {
            Kryo kryo = kryoThreadLocal.get();
            kryo.writeObject(output, obj);
            kryoThreadLocal.remove();
            return output.toBytes();

        }catch (Exception e){
            e.printStackTrace();
            throw new RuntimeException("serialize exception");
        }
    }

    @Override
    public <T> T deserialize(byte[] bytes, Class<T> clazz) {
        try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);
             Input input = new Input(byteArrayInputStream))
        {
            Kryo kryo = kryoThreadLocal.get();
            // byte->Object:从byte数组中反序列化出对对象
            Object o = kryo.readObject(input, clazz);
            kryoThreadLocal.remove();
            return clazz.cast(o);

        } catch (Exception e) {
            e.printStackTrace();
            throw new RuntimeException("deserialize exception");
        }
    }

}
```

### 注册中心

前半段是对参考项目的使用注册中心的部分进行解析，学习参考项目是如何使用注册中心的

```java
public class SocketServerMain {
    public static void main(String[] args) {
        // 在服务器端能够访问到服务的实现，new一个服务
        HelloService helloService = new HelloServiceImpl();
        // 新建一个服务器对象，服务器对象中包含两个功能：注册服务：持有注册中心对象，使用线程池监听socket连接
        SocketRpcServer socketRpcServer = new SocketRpcServer();
        // 构建service的配置类
        RpcServiceConfig rpcServiceConfig = new RpcServiceConfig();
        // 将实际对象提供给配置类
        rpcServiceConfig.setService(helloService);
        // 注册服务
        socketRpcServer.registerService(rpcServiceConfig);
        // 服务器启动
        socketRpcServer.start();
    }
}
```

服务器对象：

```java
public class SocketRpcServer {

    private final ExecutorService threadPool;
    private final ServiceProvider serviceProvider;


    public SocketRpcServer() {
        // 构造器中初始化一个线程池，这个线程池使用自己的方式进行构建
        threadPool = ThreadPoolFactoryUtil.createCustomThreadPoolIfAbsent("socket-server-rpc-pool");
        
        // 通过单例模式获取注册中心的对象
        serviceProvider = SingletonFactory.getInstance(ZkServiceProviderImpl.class);
    }

    public void registerService(RpcServiceConfig rpcServiceConfig) {
        // 将包含服务对象的配置类注册到注册中心，等于说注册中心持有一个服务的实现类
        serviceProvider.publishService(rpcServiceConfig);
    }

    public void start() {
        // 启动一个socketserver
        try (ServerSocket server = new ServerSocket()) {
            String host = InetAddress.getLocalHost().getHostAddress();
			
            server.bind(new InetSocketAddress(host, PORT));
            // ?
            CustomShutdownHook.getCustomShutdownHook().clearAll();
            Socket socket;
            while ((socket = server.accept()) != null) {
                log.info("client connected [{}]", socket.getInetAddress());
                // SocketRpcRequestHandlerRunnable即线程内运行的东西
                threadPool.execute(new SocketRpcRequestHandlerRunnable(socket));
            }
            threadPool.shutdown();
        } catch (IOException e) {
            log.error("occur IOException:", e);
        }
    }
}
```

线程内部的代码：

```java
public class SocketRpcRequestHandlerRunnable implements Runnable {
    private final Socket socket;
    private final RpcRequestHandler rpcRequestHandler;


    public SocketRpcRequestHandlerRunnable(Socket socket) {
        // 线程获取socket连接的对象
        this.socket = socket;
        // 线程获取处理服务的方法
        this.rpcRequestHandler = SingletonFactory.getInstance(RpcRequestHandler.class);
    }

    @Override
    public void run() {
        log.info("server handle message from client by thread: [{}]", Thread.currentThread().getName());
        try (ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream());
             ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream())) {
            // 从socket中读取到服务请求对象
            RpcRequest rpcRequest = (RpcRequest) objectInputStream.readObject();
            
            // 对服务请求对象进行处理
            Object result = rpcRequestHandler.handle(rpcRequest);
            // 将结果写回
            objectOutputStream.writeObject(RpcResponse.success(result, rpcRequest.getRequestId()));
            objectOutputStream.flush();
        } catch (IOException | ClassNotFoundException e) {
            log.error("occur exception:", e);
        }
    }

}
```

处理请求对象的方法：

```java
public class RpcRequestHandler {
    private final ServiceProvider serviceProvider;

    public RpcRequestHandler() {
        serviceProvider = SingletonFactory.getInstance(ZkServiceProviderImpl.class);
    }

    /**
     * Processing rpcRequest: call the corresponding method, and then return the method
     */
    public Object handle(RpcRequest rpcRequest) {
        // 从注册中心获取到提供服务的对象
        Object service = serviceProvider.getService(rpcRequest.getRpcServiceName());
        // 将请求服务的对象和提供服务的对象同时传入以反射机制为基础构建的处理方法中
        return invokeTargetMethod(rpcRequest, service);
    }

    /**
     * get method execution results
     *
     * @param rpcRequest client request
     * @param service    service object
     * @return the result of the target method execution
     */
    private Object invokeTargetMethod(RpcRequest rpcRequest, Object service) {
        Object result;
        try {
            // 反射机制调用方法
            Method method = service.getClass().getMethod(rpcRequest.getMethodName(), rpcRequest.getParamTypes());
            result = method.invoke(service, rpcRequest.getParameters());
            // 日志更新
            log.info("service:[{}] successful invoke method:[{}]", rpcRequest.getInterfaceName(), rpcRequest.getMethodName());
        } catch (NoSuchMethodException | IllegalArgumentException | InvocationTargetException | IllegalAccessException e) {
            throw new RpcException(e.getMessage(), e);
        }
        return result;
    }
}
```

跟注册中心通信了吗？

通信了，那个对象是注册中心客户端，但是通信是我们写的zookeeper客户端和zookeeper服务进行通信了，如果需要多余的业务逻辑，这些业务逻辑也是在本地实现的。

zookeeper:

采用层次化多叉树结构存储存储数据，每个节点上都可以存储数据，节点分为四类：持久节点，临时节点，持久顺序节点，临时顺序节点。

节点的信息通过stat存储，节点的数据通过data进行存储，如下为使用get查询到是节点的stat信息，以及他们分别代表什么：

![image-20230823174427702](C:\Users\yangc\AppData\Roaming\Typora\typora-user-images\image-20230823174427702.png)

三个版本信息：

- **dataVersion**：当前 znode 节点的版本号
- **cversion**：当前 znode 子节点的版本
- **aclVersion**：当前 znode 的 ACL 的版本。

ACL权限：ZooKeeper 采用 ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制

Session 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够**通过该连接接收来自服务器的 Watcher 事件通知**（是否存在访问传输数据的情况）。

sessionTimeout代表会话的超时时间，用于设定连接断开后会话多长时间过期。

session建立时服务端首先会为每个客户端都分配一个 sessionID。

为了保证高可用，最好是以集群形态来部署 ZooKeeper，ZooKeeper自身通过多台服务器提供服务，每一台服务器上的信息都是相同的，这些集群通过Raft协议进行管理，并且每台服务器之间都互相保持着通信。集群间通过 ZAB 协议（ZooKeeper Atomic Broadcast）来保持数据的一致性。

如何建立心跳机制：之前的项目并没有建立心跳机制，在服务器端设置一个关闭钩子，当服务器挂掉的时候就会直接把建立的永久节点删除，但是存在问题，如果服务器不是正常关闭的，直接发生了断网，那永久节点还是没办法删掉，还是需要心跳机制：（Curator 维持session）

Curator重试机制：https://blog.csdn.net/qq_37960603/article/details/121548596

watcher:  https://blog.csdn.net/crazymakercircle/article/details/85922561

提供服务应该是一个列表

更新线程池创建方式

服务端是否能够做到监听服务端状态的变化，或者在服务发生变化后及时更新服务节点

client从注册中心获取ip地址

RpcServiceConfig内部存储的是服务的信息，用于注册时告诉注册中心，我这个服务是什么版本的，从属于那个group，当服务器从客户端收到请求时会对这里面的信息进行核对，看看你请求的服务是不是我提供的服务

```java
public class RpcServiceConfig {
    /**
     * service version
     */
    private String version = "";
    /**
     * when the interface has multiple implementation classes, distinguish by group
     */
    private String group = "";

    /**
     * target service
     */
    private Object service;

    public String getRpcServiceName() {
        return this.getServiceName() + this.getGroup() + this.getVersion();
    }

    public String getServiceName() {
        return this.service.getClass().getInterfaces()[0].getCanonicalName();
    }
}

```

### 抽象出通信的方法

两个函数，从连接处获取一个对象，返回值为我们要处理的对象。另一个为将一个对象传递出去。

接受包括建立连接，分装对象

### 网络连接的方式

需要优化的点为：服务连接失败的场景

1。直接使用socket：这种方式的核心代码如下，首先是客户端

```java
  try (Socket socket = new Socket("localhost", 9000)) {
            // 发送对象
            ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream());
            ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream());
            objectOutputStream.writeObject(rpcRequest);
            objectOutputStream.flush();

            // 阻塞，等待responce
            RpcResponse rpcResponse = (RpcResponse) objectInputStream.readObject();

            return rpcResponse;

        } catch (IOException | ClassNotFoundException e) {
            e.printStackTrace();
            return null;
        }
```

其次是服务端：

```java
ServerSocket serverSocket = new ServerSocket(port))
System.out.println("server starting...");
Socket handleSocket;
while ((handleSocket = serverSocket.accept()) != null) {
	System.out.println("client connected, ip:" + handleSocket.getInetAddress());
    //新线程的执行
    threadPool.execute(new RpcServerWorker(handleSocket, registeredService));
}

ObjectInputStream objectInputStream = new ObjectInputStream(handleSocket.getInputStream());
ObjectOutputStream objectOutputStream = new ObjectOutputStream(handleSocket.getOutputStream());

// 接收对象
RpcRequest rpcRequest = (RpcRequest) objectInputStream.readObject();

// 发送对象
objectOutputStream.writeObject(rpcResponse);
objectOutputStream.flush();
```

这里在服务端我使用了线程池的方式对客户端信息进行监听。能够在处理多个客户端信息的情况下，还能够降低线程的创建回收成本，指定创建的最大线程数目，防止资源被浪费。

```java
 public RpcServer() {

        int corePoolSize = 5;
        int maximumPoolSize = 50;
        long keepAliveTime = 60;

        BlockingQueue<Runnable> workingQueue = new ArrayBlockingQueue<>(100);

        ThreadFactory threadFactory = Executors.defaultThreadFactory();

        this.threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.SECONDS, workingQueue, threadFactory);

        this.registeredService = new HashMap<String, Object>();
    }

// 配合上面的
threadPool.execute(new RpcServerWorker(handleSocket, registeredService));
```

对于java_guide_pcr：代码的解析

首先是客户端：在客户端中首先获得两个对象：

```java
RpcRequestTransport rpcRequestTransport = new SocketRpcClient();
RpcServiceConfig rpcServiceConfig = new RpcServiceConfig();
```

这两个对象分别是服务器对象和服务器设置对象（其中存了一些服务器的相关设置信息）与注册中心的通信就放在服务器对象中。接下来将两个对象传入RpcClientProxy，这个类为代理对象类，将要为之后传入的class提供代理类，通过代理类连接服务器，以达到rpc远程调用的效果

```java
RpcClientProxy rpcClientProxy = new RpcClientProxy(rpcRequestTransport, rpcServiceConfig);
```

传入的class 还是通过手动传入的，以下为获取代理对象的代码，该行通过传入HellowService.class来获得HellowService的代理类。

```java
HelloService helloService = rpcClientProxy.getProxy(HelloService.class);
```

在代理类中通过反射机制获取原类的各种信息构建rpcRequest，并通过调用sendRpcRequest方法与服务器进行通信：

```java
 rpcResponse = (RpcResponse<Object>) rpcRequestTransport.sendRpcRequest(rpcRequest);
```

以上为socket方式，使用Netty方式会使用CompletableFuture接受结果。服务器对象中存在一个注册中心的类serviceDiscovery，serviceDiscovery 的获取方式如下，这个获取方式是SPI的方式。

```java
this.serviceDiscovery = ExtensionLoader.getExtensionLoader(ServiceDiscovery.class).getExtension(ServiceDiscoveryEnum.ZK.getName());
// 通过lookup方式找到提供rpcRequest中要求的方法的服务器，并将服务器的地址以及端口打包成 InetSocketAddress返回
InetSocketAddress inetSocketAddress = serviceDiscovery.lookupService(rpcRequest);

//lookup方法的接口
InetSocketAddress lookupService(RpcRequest rpcRequest);
```

返回的是InetSocketAddress， InetSocketAddress类主要作用是封装端口和ip地址。



对于服务端的修改：

首先服务器测试的阶段应该隐藏新建代理等工作，测试只需要获得服务器对象，要求服务，获得对应对象，使用对象即可。

在服务器端，原本需要通过如下代码注册服务器提供的服务。这一步本来是给注册中心进行注册的，而客户端也是从注册中心拿到提供相应的功能的服务器的ip地址的。

```java
HelloService helloService = new HelloServiceImpl(); // 包含需要处理的方法的对象
rpcServer.register(helloService); // 向rpc server注册对象里面的所有方法
```



## python调用jar字典类型

要在Python中调用Java中的字典类型（也称为Map类型），可以使用Java虚拟机的Python接口——JPype。以下是一些简单的步骤，可以在Python中使用Java的字典类型：

首先，请确保已安装JPype。可以使用以下命令在命令行中安装JPype：

```
pip install JPype1
```

然后，需要导入JPype包和Java类。假设要调用Java中的HashMap类，可以使用以下代码导入：

```python
import jpype
from jpype import java
HashMap = jpype.JClass('java.util.HashMap')
```

创建一个HashMap实例：

```python
my_map = HashMap()
```

将键值对添加到HashMap中：

```python
my_map.put("key1", "value1")
my_map.put("key2", "value2")
```

使用如下代码对键值进行检索：

```python
value = my_map.get("key1")
```

最后，使用以下代码关闭Java虚拟机：

```python
jpype.shutdownJVM()
```

